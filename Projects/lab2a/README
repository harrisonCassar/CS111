NAME: Harrison Cassar
EMAIL: Harrison.Cassar@gmail.com
ID: 505114980

Included Files:
-lab2_add.c: a C program that implements and tests a shared variable add function, implements various yield/synchronization command-line options, and produces output statistics for the executions.
-SortedList.h: header file describing the interfaces for a doubly linked list's operations
-SortedList.c: a C module that implements insert, delete, lookup, and length methods for a sorted doubly linked list (described in the SortedList.h header file, including correct placement of yield calls).
-lab2_list.c: a C program that implements and tests operations done on a shared linked list, implements various yield/synchronization command-line options, and produces output statistics for the executions.
-Makefile: Makefile to build the deliverable programs (lab2_add and lab2_list), output, graphs, and tarball. Targets:
	-build: (default target) compile all programs
	-tests: run all (over 200) specified test cases to generate results in CSV files
	-graphs: uses gnuplot(1) and the included data reduction scripts lab2_add.gp and lab2_list.gp to generate the required graphs
-lab2_add.csv: contains all the statistical output for the Part-1 tests
-lab2_list.csv: contains all the statistical output for the Part-2 tests
-lab2_add graphs: created by running the graphs target in the Makefile
	-lab2_add-1.png: threads and iterations required to generate a failure (with and without yields)
	-lab2_add-2.png: average time per operation with and without yields.
	-lab2_add-3.png: average time per (single threaded) operation vs. the number of iterations.
	-lab2_add-4.png: threads and iterations that can run successfully with yields under each of the synchronization options.
	-lab2_add-5.png: average time per (protected) operation vs. the number of threads.
-lab2_list graphs: created by running the graphs target in the Makefile
	-lab2_list-1.png: average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
	-lab2_list-2.png: threads and iterations required to generate a failure (with and without yields).
	-lab2_list-3.png: iterations that can run (protected) without failure.
	-lab2_list-4.png: (length-adjusted) cost per operation vs the number of threads for the various synchronization options.
-lab2_add.gp: provided data reduction script that generates the required graphs
-lab2_list.gp: provided data reduction script that generates the required graphs

Resources:
-provided pthreads tutorial: https://computing.llnl.gov/tutorials/pthreads/
-man pages for clock_gettime(2)
-documentation for GCC atomic builtins
-man pages and user manual for gnuplot(1)

/* Questions */

QUESTION 2.1.1 - causing conflicts:
Results:
add-none,2,100,400,189765,474,0
add-none,4,100,800,221959,277,0
add-none,8,100,1600,340446,212,0
add-none,12,100,2400,441642,184,0

add-none,2,1000,4000,201990,50,0
add-none,4,1000,8000,185701,23,0
add-none,8,1000,16000,333162,20,-518
add-none,12,1000,24000,421701,17,0

add-none,2,10000,40000,599449,14,31
add-none,4,10000,80000,1266121,15,5249
add-none,8,10000,160000,2345221,14,4881
add-none,12,10000,240000,3488898,14,196

add-none,2,100000,400000,4096774,10,26039
add-none,4,100000,800000,11884822,14,-8656
add-none,8,100000,1600000,22179917,13,-8745
add-none,12,100000,2400000,32810603,13,-27634

Why does it take many iterations before errors are seen?
-The time it takes to create a thread may be greater than the time it takes the previously newly-created thread to finish its execution (therefore, guarenteed no conflicts). Additionally, the chances of a context switch occuring right in between the two instructions within the given "add()" function is really small, and therefore by increasing the number of iterations, we have a higher chance of observing this race condition.

Why does a significantly smaller number of iterations so seldom fail?
-Significantly smaller number of iterations so seldom fails because less iterations means less liklihood to fail. More iterations mean a longer execution time, and therefore more threads are in the "ready" state for execution, competing for execution time on the processor. Subsequently, more threads competing means more potential issue of conflicts when accessing the shared "counter" variable.


QUESTION 2.1.2 - cost of yielding:
Results:
add-yield-none,2,100,400,381924,954,12
add-yield-none,4,100,800,402530,503,83
add-yield-none,8,100,1600,525119,328,130
add-yield-none,12,100,2400,614148,255,233

add-yield-none,2,1000,4000,2024379,506,31
add-yield-none,4,1000,8000,2098028,262,100
add-yield-none,8,1000,16000,2250864,140,142
add-yield-none,12,1000,24000,2807647,116,202

add-yield-none,2,10000,40000,18782457,469,-79
add-yield-none,4,10000,80000,19359930,241,301
add-yield-none,8,10000,160000,20416100,127,87
add-yield-none,12,10000,240000,24752942,103,138

add-yield-none,2,100000,400000,184221752,460,-5317
add-yield-none,4,100000,800000,190289307,237,-2224
add-yield-none,8,100000,1600000,200066427,125,-7438
add-yield-none,12,100000,2400000,255871683,106,-16739

Why are the --yield runs so much slower?
-The runs with the --yield option specified are much slower simply because of the overhead that is associated with context switching for threads. Without the --yield option, there only is guarenteed to be a minimum number of context switches equal to the number of threads, whereas the guarenteed minimum number of context switches when the --yield option is specified is 2*numThreads*numIterations + numThreads (required context switch in every add() call, and add() is called twice for every iteration, as well as one more to context switch after the thread finishes execution).

Where is the additional time going?
-Like mentioned in the previous answer, the additional time is going to the overhead associated with the extra required context switches for the threads. This overhead includes interrupting the thread, saving the state of the thread (pushing registers, saving the thread-specific stack, etc.), cleaning-up the CPU to prepare for the next thread, and then loading in the new thread.

Is it possible to get valid per-operation timings if we are using the --yield option? If so, explain how. If not, explain why not.
-No, it is not possible to get valid per-operation timings if we are using the --yield option, as the time it takes to context switch (and the actual number of times that we context switch) cannot be determined, as this depends on the system itself. To be more specific, it depends on the time it takes for the dispatcher to perform the context switch between threads.


QUESTION 2.1.3 - measurement errors:
Results:
With increasing number of iterations, the results depict a decrease in the average cost per operation.

Why does the average cost per operation drop with increasing iterations?
-As the time it takes for a thread to complete its execution (based on the number of iterations specified for it to run) increases, the time and overhead it takes for the main process to create all the threads becomes more and more insignificant. The fixed amount of time for all of these thread creations is spread out over more and more operations (which is proportional to the number of iterations), therefore the average cost per operation is inversely proportional to the number of iterations.

If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
-The "correct" cost can be estimated/"determined" by increasing the number of iterations by more and more. As the number of iterations approaches infinity, this constant fixed overhead cost for thread creation becomes so insignificant it can be seen as essentially 0 (non-contributing to the total cost).


QUESTION 2.1.4 - costs of serialization:
Results:
For synchronization runs, every single test is done without failure (as expected). Additionally, the number of threads increase, the cost of every operation increases for tests with synchronization enabled.

Why do all of the options perform similarly for low numbers of threads?
-Since there are not that many threads, the overhead cost associated with each of these options are not too apparent, as they are not performed as often. Essentially, the overall overhead cost with creating the threads (can almost be deemed "mandatory" cost) masks the overhead associated with each option until they become big enough (with adding more threads) to distinguish themselves in their per-operation-costs.

Why do the three protected operations slow down as the number of threads rises?
-As the number of threads rise, the more overhead there is associated with properly handling a thread's access to the shared data. As more threads attempt to access the same data, if it is locked, then they must wait for the data to be unlocked, thereby leading to time wasted with no work/execution being done in the process (thereby increasing the amount of per-operation-cost).


QUESTION 2.2.1 - scalability of Mutex
Compare the variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists).
Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

The variation in time per mutex-protected operation against the number of threads in both Part 1 and Part 2 increase in an approximately linear fashion. For Part 1 (the addition case), the curve begins off increasing at a high rate, but over time begins to plateau off (a lower rate of change with more threads). This is likely resulting because of the relatively small size of the add's critical section, meaning that threads who are waiting for the mutex to be unlocked will not have to wait as relatively long as in the case of Part 2 (with a linked list). In the case of Part 2, the linear curve stays increasing at a relatively stable rate, as expected. Since the operations done on the linked list are slower, thus meaning the overhead associated with mutex-protecting of the critical section is expensive, and thus would have to increase with the increase of the number of threads.


QUESTION 2.2.2 - scalability of spin locks

Compare the variation in time per protected operation vs the number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

The general trends that are depicted for both mutexes and spinlocks are that as we increase the number of threads, the time for a protected operation increases. This is expected, as there is necessary overhead to protect from race conditions appearing when operating on shared data. However, the rate of this increase in average waiting time is different between mutexes and spinlocks simply by the nature that each are implemented. As we continue to increase the number of threads, the time per protected operation using spinlocks continues to increase as well. The average waiting time for each thread when waiting for a spinlock to be unlocked is increased, as it eats up execution time while waiting for a resource to be unlocked (if it were instead blocked in the case of mutexes, its position in the CPU can be taken by another thread which could be potentially working with the resource that the first thread is waiting for, leading to less wait time). Therefore, as seen in the graphs, the per operation cost of mutexes tends to increase at a slower rate than for the spinlocks. Although spinlocks are great for their simplicity, mutexes seem to have the advantage of not wasting valuable execution time on the CPU while waiting for a resource to be available to it.
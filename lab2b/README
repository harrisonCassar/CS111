NAME: Harrison Cassar
EMAIL: Harrison.Cassar@gmail.com
ID: 505114980

Included Files:
-SortedList.h: header file describing the interfaces for a doubly linked list's operations
-SortedList.c: a C module that implements insert, delete, lookup, and length methods for a sorted doubly linked list (described in the SortedList.h header file, including correct placement of yield calls).
-lab2_list.c: a C program that implements and tests operations done on a shared linked list, implements various yield/synchronization/partitioning-list command-line options, and produces output statistics for the executions.
-Makefile: Makefile to build the deliverable programs (lab2_list), output, graphs, and tarball. Targets:
	-default: (default target) compile all programs
	-tests: run all specified test cases to generate results in CSV files
	-profile: run tests with profiling tools to generate an execution profiling report
	-graphs: uses gnuplot(1) to generate the required graphs
	-dist: create the deliverable tarball
	-clean: delete all programs and output generated by the Makefile
-lab2b_list.csv: contains all the statistical output for all of the test runs
-profile.out: execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.
-graphs:
	-lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized list operations.
	-lab2b_2.png: mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
	-lab2b_3.png: successful iterations vs. threads for each synchronization method.
	-lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists.
	-lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.
-lab2b_list.gp: script that generates the required graphs

Resources:
-provided pthreads tutorial: https://computing.llnl.gov/tutorials/pthreads/
-man pages for clock_gettime(2)
-documentation for GCC atomic builtins
-man pages and user manual for gnuplot(1)

/* Questions */

QUESTION 2.3.1 - Cycles in the basic list implementation:

Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
-Since there exists little threads to contend for execution time on the CPU core (no contention in the case of 1 thread, a little in the case of 2 threads), most of the cycles in these tasks are spent actually peforming the task. As previously outlined, the tasks includes the operations of adding elements to a list, checking the length, and removing the elements from the list.

Why do you believe these to be the most expensive parts of the code?
-Performing the actual task is the most expensive parts of the code because there is little contention for acquiring the lock on the shared list data. Therefore, this means that threads are waiting little time (if any even at all) to acquire the lock to the shared list, meaning that the long time it takes to peform the operation (O(n) for an insertion, finding the length, and finding an element by key) will completely overpower the overhead associated with lock management. 

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
-Most of the time/cycles in the high-thread spin-lock tests are spent on threads spinning during their entire allotted execution time. Threads who do not currently have the lock (and therefore cannot work on the shared list), will continue to loop over and over again, asking for access to the lock until the lock is eventually available. Since the threads do not yield when they do not successfully obtain the lock when first asking, all of this time is wasted, as the thread who potentially has the lock cannot execute as quickly (therefore cannot release the lock until much later).

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
-Most of the time/cycles in the high-thread mutex tests are spent on the overhead associated with context switching between threads, as whenever a thread is context switched into the CPU core, it will ask for the lock. If it is unable to receive it (as only one thread can obtain the lock at a time), then it yields, leading to another context switch to another thread. This other thread, however, may not be the one with the lock either, therefore leading to another context switch. Eventually, the thread with the lock is able to execute its operations on the shared list, and eventually releases the lock for the same process to repeat again (but by this time, most of the time has been used on context switching from thread to thread).


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
-The lines of code that are consuming most of the cycles with a large number of threads is the portion of the code within the actuall spinning-waiting implementation. This is specifically on lines 435, 477, and 543 of the lab2_list.c source code.

Why does this operation become so expensive with large numbers of threads?
-This operation becomes so expensive with a large number of threads simply because the full length of execution time that a thread is allotted is wasted "spinning" while waiting for a different thread (which potentially isn't even running) to release the lock on the shared data. Since the spinning thread does not yield/block itself, its execution time is essentially just wasted (this time could have been used to allow the thread currently holding the lock to potentially finish its operation on the shared data, thus leading to an unlocking of the lock it holds).


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
-The reason that the average lock-wait time rises so dramatically with the number of contending threads is because the more threads you add to contend for the lock, the more time that a specific blocked thread must wait to get access to the shared data (on average). Since there are more threads, this average continues to rise, as more threads will be waiting for several more threads before it to finish their operation on the shared data (and release the lock).

Why does the completion time per operation rise (less dramatically) with the number of contending threads?
-The completion time per operation rises (less) dramatically with the number of contending threads because the use of mutexes means that threads who do not own the mutex lock to the shared data block themselves until they can have access to the mutex lock. Therefore, unlike spinlocks, cycles are not wasted spinning. Instead, the only time wasted is context switching out threads that do not have access to the mutex, which does add time to the average time per operation with more threads, as more threads must be context switched out and in.

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
-I assume "wait time per operation" means the wait time it takes to begin an operation (thus implying the wait-for-mutex time). The reason it's possible for the wait time per operation to rise faster than the completion time per operation is simply because the waiting time per operation is accounting for all of the threads that are blocked during a cyle of execution. Therefore the wait time increases faster because it is working with a sum over all waiting threads, while the completion time per operation is working with all operations being executed over all threads (doesn't matter when the operations are done, but instead considering total time from operation start to end, dividing this by the total number of operations).


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
-The change in performance of each the mutex and spinlock synchronization methods as a function of the number of lists should be generally increased, as the more lists that exist, the more parallelism that can be exploited, as threads do not need to contend as much for a specific shared sublist (accessing concurrently). Increased parallelism means increased peformance--up to a point (explained in next answer).

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
-The throughput should not continue to increase as the number of lists is further increased because eventually increasing the lists to a certain point will lead to close to 0 contention, therefore meaning that any excess lists from there on out will only lead to more overhead associated with managing locks (will not increase parallelism any more). Thus, throughput would decrease, as more overhead is introduced without improving our "valuable" operations. This "threshhold" value seems to be at consistently about when we have the same number of threads and lists.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
-This does not appear true in the curves generated, as an N-way partitioned list allows for more parallelism to be exploited, thus improving throughput (despite the higher overhead with managing more locks). A single list with fewer threads, although less overhead is associated with lock management (less locks to manage in the first place), less operations can be done within a certain span of time, as numerous threads cannot work concurrently on the shared list data.
